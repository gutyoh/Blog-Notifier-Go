<h2>Description</h2>

<p>A web crawler is an invaluable tool for users who want to keep an eye on new content, references, or resources that emerge on a blog. In this stage, you will build a CLI sub-command and create a program that can crawl a website recursively to a specified depth, collecting links and presenting them in a user-friendly manner.</p>

<h2>Objectives</h2>

<p>Your task is to implement the <code>--crawl-site</code> flag for the Blog Notifier CLI; this command should accept a website URL as input, and the program needs to crawl this website and find all the hyperlinks on the site recursively up to a maximum depth of 3. The program should output a list of unique hyperlinks found, excluding any duplicates that may have been encountered.</p>

<p>In Go, you can fetch and parse HTML files using the <code>net/http</code> and <code>goquery</code> packages; the process typically involves fetching the website using its URL and then using methods from the <code>goquery</code> package to find all the hyperlinks. Below is a simplified example to illustrate this process.</p>

<p>First, you need to fetch the website by making an HTTP get request to its URL with the <code>http.Get()</code> function from the <code>net/http</code> package:</p>

<pre><code class="language-go">// Make an HTTP GET request to the specified website
res, err := http.Get(site)

if err != nil {
    return nil, fmt.Errorf("could not reach the site: %s", site)
}

defer res.Body.Close()

// Check if the HTTP response status code is not 200 (OK)
if res.StatusCode != 200 {
    return nil, err
}</code></pre>

<p>Next, to find all the hyperlinks inside a given HTML, you'll need to use a few methods from the <a href="https://pkg.go.dev/github.com/PuerkitoBio/goquery" rel="noopener noreferrer nofollow" target="_blank">goquery package</a>:</p>

<pre><code class="language-go">// Load the HTML document
doc, err := goquery.NewDocumentFromReader(res.Body)
if err != nil {
    return nil, err
}

// Initialize an empty slice to store discovered links
links := make([]string, 0)

// Iterate over all 'a' (anchor) elements in the HTML document
doc.Find("a").Each(func(i int, s *goquery.Selection) {

    // Extract the 'href' attribute value from each 'a' element
    link, exists := s.Attr("href")

    if exists {
        // Add the discovered link to the slice
        links = append(links, link)
    }
})</code></pre>

<p>In the above snippet, the first step is to create a new <a href="https://pkg.go.dev/github.com/PuerkitoBio/goquery#Document" rel="noopener noreferrer nofollow"><code>goquery</code> document</a> from the response body (<code>res.Body</code>) using the <code>goquery.NewDocumentFromReader</code> function.</p>

<p>Once the HTML is loaded into the <code>goquery</code> document (<code>doc</code>) we use the <code>doc.Find("a")</code> method to iterate over all anchor (<code>&lt;a&gt;</code>) elements in the HTML, extracting the <code>href</code> attribute from each, which represents a hyperlink. The hyperlinks are then stored in a slice, ensuring that all discovered links from the webpage get captured; this approach forms the basis of the crawler, and integrating the above process into your CLI tool will enable it to collect and list hyperlinks from a given website effectively.</p>

<h2>Examples</h2>

<p><strong>Example 1: Crawling a blog site with multiple pages.</strong></p>

<p>Below is the expected output of the <code>--crawl-site</code> command:</p>

<pre><code class="language-no-highlight">blognotifier --crawl-site "https://brianpzaide.github.io/blog-notifier"
https://brianpzaide.github.io/blog-notifier/a.html
https://brianpzaide.github.io/blog-notifier/b.html
https://brianpzaide.github.io/blog-notifier/c.html
https://brianpzaide.github.io/blog-notifier/d.html
https://brianpzaide.github.io/blog-notifier/e.html
https://brianpzaide.github.io/blog-notifier/f.html
</code></pre>

<p><strong>Example 2: Crawling a blog site with no hyperlinks.</strong></p>

<p>Below is the expected output of the <code>--crawl-site</code> command for the a that does NOT have any blog posts:</p>

<pre><code class="language-no-highlight">blognotifier --crawl-site "http://noblogpostsyet.com"
No blog posts found for http://noblogpostsyet.com
</code></pre>

<p>⚠️ Note that the tests will check the new <code>--crawl-site</code> command functionality on a simple website. You can find a sample of the website <a href="https://brianpzaide.github.io/blog-notifier" rel="noopener noreferrer nofollow">here</a>.</p>